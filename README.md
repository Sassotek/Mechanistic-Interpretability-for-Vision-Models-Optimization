# Mechanistic-Interpretability-for-Vision-Models-Optimization
<!-- Badges affiancati -->
[![Camilla Giuliani on GitHub](https://img.shields.io/badge/Camillaâ€“Giulianiâ€“GitHub-181717?style=flat-square&logo=github)](https://github.com/camygiuliani)
[![Pietro D'Annibale on GitHub](https://img.shields.io/badge/Pietroâ€“D%E2%80%99Annibaleâ€“GitHub-181717?style=flat-square&logo=github)](https://github.com/Sassotek)


Project made for 2025 Computer Vision's course by Pietro D'Annibale, 1917211, and Camilla Giuliani, 1883207.

---

## Table of Contents ðŸ“–

1. [Colab notebook](#colab-notebook)  
2. [Dataset](#dataset)  
3. [Model](#model)  
4. [Usage](#usage)  
5. [Training](#training)  
6. [Evaluation & Results](#evaluation--results)  
7. [Pruning con ACDC](#pruning-con-acdc)  
8. [Interpretability](#interpretability)  
9. [Contributing](#contributing)  
10. [License](#license)  
11. [References](#references)  

---

## Colab notebook
Our notebbok can be accessed via the following link:
```bash
https://colab.research.google.com/drive/1NNMyHI6ySeZPHcacPNtQd6y8-yUvGMZX

```

## Dataset
Tiny-ImageNet downloaded by https://www.kaggle.com/datasets/wissamsalam/tiny-imagenet-cleaned-for-classification
## License
MIT License...

## References 
- [A. Conmy et al. (2023). Towards Automated Circuit Discovery for Mechanistic Interpretability. In: Advances
in Neural Information Processing Systems 36 (NeurIPS 2023)](https://arxiv.org/abs/2304.14997)
- [A. Syed, C. Rager and A.Conmy, (2024). Attribution Patching Outperforms Automated Circuit Discovery,
BlackboxNLP 2024.](https://arxiv.org/abs/2310.10348) 