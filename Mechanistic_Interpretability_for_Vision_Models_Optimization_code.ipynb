{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sassotek/Mechanistic-Interpretability-for-Vision-Models-Optimization/blob/main/Mechanistic_Interpretability_for_Vision_Models_Optimization_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notes"
      ],
      "metadata": {
        "id": "6jzzOI7xEby3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########Dataset Description\n",
        "File descriptions\n",
        "\n",
        "    train.images.zip - the training set (images distributed into class labeled folders)\n",
        "    test.zip - the unlabeled 10,000 test images\n",
        "   \n",
        "    wnids.txt - list of the used ids from the original full set of ImageNet\n",
        "    words.txt - description of all ids of ImageNet\n",
        "\n",
        "\n",
        "+++++ https://www.kaggle.com/datasets/wissamsalam/tiny-imagenet-cleaned-for-classification\n",
        "\n",
        "\n",
        "https://viso.ai/deep-learning/vision-transformer-vit/\n",
        "\n",
        "https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch\n",
        "\n",
        "https://docs.pytorch.org/vision/main/models/vision_transformer.html\n",
        "\n",
        "https://github.com/lucidrains/vit-pytorch\n",
        "\n",
        "https://arxiv.org/pdf/2010.11929.pdf\n",
        "\n",
        "https://www.youtube.com/watch?v=j3VNqtJUoz0\n",
        "\n",
        "https://www.youtube.com/watch?v=vJF3TBI8esQ\n",
        "\n",
        "https://www.youtube.com/watch?v=nZ22Ecg9XCQ\n",
        "\n",
        "**link sbagliato**\n",
        "\n",
        "https://www.kaggle.com/c/tiny-imagenet\n",
        "\n",
        "#####GuidaEinops\n",
        "\n",
        "https://nbviewer.org/github/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb"
      ],
      "metadata": {
        "id": "8DEOK911Eext"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**latent_size** (o anche detto embedding_dim, hidden_dim, ecc.) Ã¨ la dimensione del vettore che rappresenta ogni patch dopo la proiezione lineare, ovvero la dimensione dello spazio latente in cui il modello \"lavora\".\n",
        "\n",
        "**class token** per ogni batch, un vettore speciale che sarÃ  usato dal ViT per l'output della classificazione.\n",
        "\n",
        "**positional embedding** iniziale. VerrÃ  poi ripetuta per il numero di patch + 1 (per il class token). Serve a dare informazioni sulla posizione dei patch nel Transformer.\n",
        "\n",
        "**patches = einops.rearrange**(\n",
        "    input,\n",
        "    'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
        "    p1=self.patch_size, p2=self.patch_size\n",
        ")*\n",
        "Usa Einops per dividere ogni immagine in patch:\n",
        "\n",
        "    Input: (b, c, H, W)\n",
        "\n",
        "    Output: (b, N_patch, patch_dim), dove:\n",
        "\n",
        "        N_patch = (H // patch_size) * (W // patch_size)\n",
        "\n",
        "        patch_dim = patch_size * patch_size * c\n",
        "\n",
        "Esempio: immagine 64x64, patch_size=16 â†’ 16 patch da 16x16x3"
      ],
      "metadata": {
        "id": "GaUn_8CcdgfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save_path_model= ................./ciao.pth\n",
        "#save_path_opt= ..../ott.pth"
      ],
      "metadata": {
        "id": "EralO6LRwqIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "D8G8Wllzv7RG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0BiaX_tGip4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59ea6bf-877f-4598-ebef-779684361ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device cuda\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage import io, color\n",
        "\n",
        "#Kaggle\n",
        "!pip install kagglehub --quiet\n",
        "import kagglehub\n",
        "\n",
        "#PyTorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList\n",
        "from torch.utils.data import Dataset,DataLoader, SubsetRandomSampler, ConcatDataset\n",
        "import torchvision\n",
        "from torchvision import transforms,datasets\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import os\n",
        "import pandas as pd..\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import seaborn as sns\n",
        "import einops\n",
        "import sys\n",
        "import requests\n",
        "import urllib.request\n",
        "from io import BytesIO\n",
        "from prettytable import PrettyTable\n",
        "from scipy import signal\n",
        "from scipy.fft import fft, fftfreq, fftshift\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "from IPython.display import HTML\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, IntSlider, FloatSlider, FloatRangeSlider, Dropdown\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "\n",
        "## GDrive settings\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Setting the seed\n",
        "torch.manual_seed(240700)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device\",device)\n",
        "if torch.cuda.is_available():\n",
        "   torch.cuda.manual_seed(240700)\n",
        "   torch.cuda.manual_seed_all(240700)\n",
        "\n",
        "#ensure that all operations are deterministic on GPU if used,for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globals"
      ],
      "metadata": {
        "id": "fHCbuzIqFd50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_file_path=\"/content/drive/File_projects_CV\"\n",
        "\n",
        "RED = \"\\033[31m\"\n",
        "GREEN = \"\\033[32m\"\n",
        "BLUE = \"\\033[34m\"\n",
        "RESET = \"\\033[0m\"\n",
        "CYAN=\"\\033[36m\"\n",
        "MAGENTA=\"\\033[35m\"\n",
        "\n",
        "#Transformer hyperparameters\n",
        "patch_size = 16\n",
        "latent_size = 768\n",
        "n_channels = 3\n",
        "n_heads = 12\n",
        "n_encoders = 12\n",
        "dropout = 0.1\n",
        "n_classes = 200 #See tinyimagenet classes\n",
        "image_size = 64 #See tinyimagenet classes\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 4\n",
        "threshold = 10e-3\n",
        "weight_decay = 0.03"
      ],
      "metadata": {
        "id": "uimoN7OJFgXo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "bFwpDmIWVWxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_path = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n",
        "data_path= kagglehub.dataset_download(\"wissamsalam/tiny-imagenet-cleaned-for-classification\")\n",
        "data_path= data_path+'/tiny-imagenet-200/'\n",
        "print(\"Path to dataset files:\", data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YoeVA5EcadQ",
        "outputId": "23b05cfc-424f-4cac-b723-a64e06f1dd93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/tiny-imagenet-cleaned-for-classification/tiny-imagenet-200/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_dataset_structure(root_dir, max_classes=3, max_images=3):\n",
        "    print(f\"ðŸ“ Directory: {root_dir}\\n\")\n",
        "\n",
        "    root_items = sorted(os.listdir(root_dir))\n",
        "    for item in root_items:\n",
        "        item_path = os.path.join(root_dir, item)\n",
        "\n",
        "        if os.path.isfile(item_path):\n",
        "            print(f\"ðŸ“„ {item}\")\n",
        "\n",
        "        elif os.path.isdir(item_path) and item in ['train', 'val', 'test']:\n",
        "            print(f\"\\nðŸ“‚ {item}/\")\n",
        "\n",
        "            class_dirs_all = sorted([\n",
        "                d for d in os.listdir(item_path)\n",
        "                if os.path.isdir(os.path.join(item_path, d))\n",
        "            ])\n",
        "            class_dirs = class_dirs_all[:max_classes]\n",
        "\n",
        "            if not class_dirs:\n",
        "                print(f\"  (No subdirectory found into {item}/)\")\n",
        "                continue\n",
        "\n",
        "            for cls in class_dirs:\n",
        "                cls_path = os.path.join(item_path, cls)\n",
        "                print(f\"  â”œâ”€â”€ {cls}/\")\n",
        "\n",
        "                image_files_all = sorted([\n",
        "                    f for f in os.listdir(cls_path)\n",
        "                    if os.path.isfile(os.path.join(cls_path, f))\n",
        "                ])\n",
        "                image_files = image_files_all[:max_images]\n",
        "\n",
        "                for img in image_files:\n",
        "                    print(f\"  â”‚   â”œâ”€â”€ {img}\")\n",
        "\n",
        "                if len(image_files_all) > max_images:\n",
        "                    print(f\"  â”‚   â””â”€â”€ ...\")\n",
        "\n",
        "            if len(class_dirs_all) > max_classes:\n",
        "                print(f\"  â””â”€â”€ ...\")\n",
        "\n",
        "\n",
        "print_dataset_structure(data_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97i9LMQSV1vS",
        "outputId": "f1742650-ce27-41c0-f81f-78b41d4bfe84"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Directory: /kaggle/input/tiny-imagenet-cleaned-for-classification/tiny-imagenet-200/\n",
            "\n",
            "\n",
            "ðŸ“‚ test/\n",
            "  â”œâ”€â”€ n01443537/\n",
            "  â”‚   â”œâ”€â”€ n01443537_0.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01443537_1.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01443537_101.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â”œâ”€â”€ n01629819/\n",
            "  â”‚   â”œâ”€â”€ n01629819_0.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01629819_1.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01629819_10.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â”œâ”€â”€ n01641577/\n",
            "  â”‚   â”œâ”€â”€ n01641577_0.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01641577_1.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01641577_104.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â””â”€â”€ ...\n",
            "\n",
            "ðŸ“‚ train/\n",
            "  â”œâ”€â”€ n01443537/\n",
            "  â”‚   â”œâ”€â”€ n01443537_10.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01443537_100.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01443537_102.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â”œâ”€â”€ n01629819/\n",
            "  â”‚   â”œâ”€â”€ n01629819_100.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01629819_101.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01629819_102.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â”œâ”€â”€ n01641577/\n",
            "  â”‚   â”œâ”€â”€ n01641577_10.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01641577_100.JPEG\n",
            "  â”‚   â”œâ”€â”€ n01641577_101.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â””â”€â”€ ...\n",
            "\n",
            "ðŸ“‚ val/\n",
            "  â”œâ”€â”€ n01443537/\n",
            "  â”‚   â”œâ”€â”€ val_1230.JPEG\n",
            "  â”‚   â”œâ”€â”€ val_1267.JPEG\n",
            "  â”‚   â”œâ”€â”€ val_1284.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â”œâ”€â”€ n01629819/\n",
            "  â”‚   â”œâ”€â”€ val_1054.JPEG\n",
            "  â”‚   â”œâ”€â”€ val_1167.JPEG\n",
            "  â”‚   â”œâ”€â”€ val_1535.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â”œâ”€â”€ n01641577/\n",
            "  â”‚   â”œâ”€â”€ val_1144.JPEG\n",
            "  â”‚   â”œâ”€â”€ val_1251.JPEG\n",
            "  â”‚   â”œâ”€â”€ val_130.JPEG\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â””â”€â”€ ...\n",
            "ðŸ“„ wnids.txt\n",
            "ðŸ“„ words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##function to check dim and stuff"
      ],
      "metadata": {
        "id": "F7O4cB-xzMO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Network"
      ],
      "metadata": {
        "id": "ETsYuzUcZqV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input Embedding"
      ],
      "metadata": {
        "id": "rwi-y45-kMW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size=patch_size, n_channels=n_channels, latent_size=latent_size,\n",
        "                batch_size=1, device=device):\n",
        "        super(InputEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.n_channels = n_channels\n",
        "        self.latent_size = latent_size\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        self.input_size = self.patch_size*self.patch_size*self.n_channels\n",
        "        self.LinearProjection = nn.Linear(self.input_size, self.latent_size) #Linear projection\n",
        "        self.class_token = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device) #Class token\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device) #Positional embedding\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.to(self.device)\n",
        "\n",
        "        #Patchification of input image\n",
        "        patches = einops.rearrange(\n",
        "            input,\n",
        "            'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
        "            p1=self.patch_size, p2=self.patch_size\n",
        "            )\n",
        "\n",
        "        print(input.size())\n",
        "        print(patches.size())\n",
        "\n",
        "        LinearProjection = self.LinearProjection(patches).to(self.device)\n",
        "        batch, n_patches, _ = LinearProjection.shape\n",
        "        LinearProjection = torch.cat((self.class_token, LinearProjection), dim=1)\n",
        "\n",
        "        #controllare erpchÃ© tutti i token di ogni immagine hanno la stessa positional embedding!\n",
        "        positional_embedding = einops.repeat(\n",
        "            self.positional_embedding,\n",
        "            'b 1 d -> b m d',\n",
        "            m=n_patches+1\n",
        "            )\n",
        "\n",
        "        print(LinearProjection.size())\n",
        "        print(positional_embedding.size())\n",
        "\n",
        "        LinearProjection += positional_embedding\n",
        "        return LinearProjection"
      ],
      "metadata": {
        "id": "DVHGNPtzn9ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ],
      "metadata": {
        "id": "YpTUixdaX1Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_size=latent_size, n_heads=n_heads, dropout=dropout, device = device):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "\n",
        "        #norm layer\n",
        "        self.norm = nn.LayerNorm(self.latent_size)\n",
        "\n",
        "        #multihead atention\n",
        "        self.multihead_attention = nn.MultiheadAttention(self.latent_size, self.n_heads, dropout=self.dropout)\n",
        "\n",
        "        #add input\n",
        "        #norm\n",
        "        #MLP\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(self.latent_size, self.latent_size*4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.latent_size*4, self.latent_size),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, embedded_patches):\n",
        "        first_norm_out = self.norm(embedded_patches)\n",
        "        attention_out = self.multihead_attention(first_norm_out, first_norm_out, first_norm_out)[0]\n",
        "\n",
        "        first_add = attention_out + embedded_patches\n",
        "\n",
        "        second_norm_out = self.norm(first_add)\n",
        "        MLP_out = self.MLP(second_norm_out)\n",
        "\n",
        "        out = MLP_out + first_add\n",
        "\n",
        "        #print('embed: ', embedded_patches.size())\n",
        "        #print('output', out.size())\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "tManjtTzX28D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer"
      ],
      "metadata": {
        "id": "7MpsVRm3cb5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(put everything together)"
      ],
      "metadata": {
        "id": "rQc4anC3fZHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vit(nn.Module):\n",
        "    def __init__(self, n_encoders=n_encoders, latent_size=latent_size, n_classes=n_classes, dropout = dropout, device=device):\n",
        "        super(Vit, self).__init__()\n",
        "        self.n_encoders = n_encoders\n",
        "        self.latent_size = latent_size\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "\n",
        "        self.Embedding = InputEmbedding()\n",
        "\n",
        "        self.EncoderStack = nn.ModuleList([Encoder() for i in range(self.n_encoders)])\n",
        "\n",
        "        self = MLPHead = nn.Sequential(\n",
        "            nn.LayerNorm(self.latent_size),\n",
        "            nn.Linear(self.latent_size, self.latent_size),\n",
        "            nn.Linear(self.latent_size, self.n_classes)\n",
        "        )\n",
        "\n",
        "    def Forward(self, input):\n",
        "        encoder_out = self.Embedding(input)\n",
        "\n",
        "        for enc in self.EncoderStack:\n",
        "            encoder_out = enc(encoder_out)\n",
        "\n",
        "        cls_token = encoder_out[:, 0]\n",
        "        MLPHead_out = self.MLPHead(cls_token)\n",
        "\n",
        "        return MLPHead_out\n",
        "\n"
      ],
      "metadata": {
        "id": "RWtzx0svclOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = torch.randn(1,3,64,64)\n",
        "test_class = InputEmbedding().to(device)\n",
        "test_class(test_input)\n",
        "\n",
        "test_encoder = Encoder().to(device)\n",
        "test_encoder(test_class(test_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1KzMlDFrneJ",
        "outputId": "e31757b9-44ee-4d05-c40a-081fa9d437f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 64, 64])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 3, 64, 64])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "embed:  torch.Size([1, 17, 768])\n",
            "output torch.Size([1, 17, 768])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.8716, -1.0013, -1.4098,  ...,  0.8561, -0.5967,  0.6804],\n",
              "         [-0.9453,  0.2115, -1.2030,  ..., -1.0250, -1.8071,  2.2308],\n",
              "         [-0.9233,  0.0763, -0.2545,  ..., -0.3028, -2.5491,  0.7847],\n",
              "         ...,\n",
              "         [-1.6709, -1.4999, -0.9490,  ..., -0.5014, -0.5639,  0.9106],\n",
              "         [-0.6949, -0.4929, -0.3836,  ..., -1.6464, -2.2312,  0.9835],\n",
              "         [-0.7364, -1.2531, -1.0976,  ..., -1.9165, -2.3608,  1.7134]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}